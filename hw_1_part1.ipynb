{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_1_part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRVYouzuRETn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyTmOXupRUT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder =  LabelEncoder()\n",
        "y1 = encoder.fit_transform(y)\n",
        "Y = pd.get_dummies(y1).values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svacABgDRU74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "befbdebb-0e10-4e81-d5b9-df5c07f7a054"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZpdbRyXRYEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbffaa33-8e37-4ee9-e429-6d7296fe40f4"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db4aHcdjYtfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=228) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ6grl_7UfPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "45d8120b-5e7e-4272-8e44-6ee0ac987b77"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "def build_model():\n",
        "  model = Sequential([\n",
        "      Dense(16, input_shape=(4,),activation='tanh'),\n",
        "      Dense(32,activation='tanh'),\n",
        "      Dense(3,activation='softmax'),\n",
        "  ])\n",
        "  model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLG_EeIUTyC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "6efdd25e-0a5a-4108-c5cb-0da7fc1deb94"
      },
      "source": [
        "model1 = build_model()\n",
        "model1.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 723\n",
            "Trainable params: 723\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tQWp_E0VhgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "926bec41-591d-4f13-9305-769babe12ee9"
      },
      "source": [
        "\n",
        "model1.fit(X_train,y_train,epochs = 100)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "120/120 [==============================] - 1s 6ms/step - loss: 1.4601 - acc: 0.3500\n",
            "Epoch 2/100\n",
            "120/120 [==============================] - 0s 83us/step - loss: 1.3354 - acc: 0.3500\n",
            "Epoch 3/100\n",
            "120/120 [==============================] - 0s 80us/step - loss: 1.2191 - acc: 0.3500\n",
            "Epoch 4/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 1.1307 - acc: 0.3500\n",
            "Epoch 5/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 1.0617 - acc: 0.3750\n",
            "Epoch 6/100\n",
            "120/120 [==============================] - 0s 87us/step - loss: 1.0110 - acc: 0.6667\n",
            "Epoch 7/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.9766 - acc: 0.7000\n",
            "Epoch 8/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.9561 - acc: 0.7000\n",
            "Epoch 9/100\n",
            "120/120 [==============================] - 0s 91us/step - loss: 0.9251 - acc: 0.7000\n",
            "Epoch 10/100\n",
            "120/120 [==============================] - 0s 87us/step - loss: 0.8929 - acc: 0.7000\n",
            "Epoch 11/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.8609 - acc: 0.7000\n",
            "Epoch 12/100\n",
            "120/120 [==============================] - 0s 71us/step - loss: 0.8267 - acc: 0.7000\n",
            "Epoch 13/100\n",
            "120/120 [==============================] - 0s 71us/step - loss: 0.7924 - acc: 0.7000\n",
            "Epoch 14/100\n",
            "120/120 [==============================] - 0s 78us/step - loss: 0.7608 - acc: 0.7000\n",
            "Epoch 15/100\n",
            "120/120 [==============================] - 0s 75us/step - loss: 0.7291 - acc: 0.7000\n",
            "Epoch 16/100\n",
            "120/120 [==============================] - 0s 68us/step - loss: 0.6996 - acc: 0.7000\n",
            "Epoch 17/100\n",
            "120/120 [==============================] - 0s 65us/step - loss: 0.6723 - acc: 0.7000\n",
            "Epoch 18/100\n",
            "120/120 [==============================] - 0s 67us/step - loss: 0.6451 - acc: 0.7000\n",
            "Epoch 19/100\n",
            "120/120 [==============================] - 0s 67us/step - loss: 0.6216 - acc: 0.7000\n",
            "Epoch 20/100\n",
            "120/120 [==============================] - 0s 89us/step - loss: 0.5990 - acc: 0.7167\n",
            "Epoch 21/100\n",
            "120/120 [==============================] - 0s 95us/step - loss: 0.5807 - acc: 0.7083\n",
            "Epoch 22/100\n",
            "120/120 [==============================] - 0s 90us/step - loss: 0.5608 - acc: 0.7167\n",
            "Epoch 23/100\n",
            "120/120 [==============================] - 0s 77us/step - loss: 0.5431 - acc: 0.7333\n",
            "Epoch 24/100\n",
            "120/120 [==============================] - 0s 76us/step - loss: 0.5269 - acc: 0.7667\n",
            "Epoch 25/100\n",
            "120/120 [==============================] - 0s 75us/step - loss: 0.5123 - acc: 0.8083\n",
            "Epoch 26/100\n",
            "120/120 [==============================] - 0s 78us/step - loss: 0.4983 - acc: 0.8167\n",
            "Epoch 27/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.4863 - acc: 0.8667\n",
            "Epoch 28/100\n",
            "120/120 [==============================] - 0s 91us/step - loss: 0.4737 - acc: 0.9167\n",
            "Epoch 29/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.4625 - acc: 0.9167\n",
            "Epoch 30/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.4503 - acc: 0.8917\n",
            "Epoch 31/100\n",
            "120/120 [==============================] - 0s 92us/step - loss: 0.4395 - acc: 0.9167\n",
            "Epoch 32/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.4294 - acc: 0.9250\n",
            "Epoch 33/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.4186 - acc: 0.9417\n",
            "Epoch 34/100\n",
            "120/120 [==============================] - 0s 91us/step - loss: 0.4078 - acc: 0.9250\n",
            "Epoch 35/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.3977 - acc: 0.9417\n",
            "Epoch 36/100\n",
            "120/120 [==============================] - 0s 88us/step - loss: 0.3895 - acc: 0.9250\n",
            "Epoch 37/100\n",
            "120/120 [==============================] - 0s 82us/step - loss: 0.3780 - acc: 0.9500\n",
            "Epoch 38/100\n",
            "120/120 [==============================] - 0s 85us/step - loss: 0.3689 - acc: 0.9583\n",
            "Epoch 39/100\n",
            "120/120 [==============================] - 0s 88us/step - loss: 0.3603 - acc: 0.9500\n",
            "Epoch 40/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.3522 - acc: 0.9583\n",
            "Epoch 41/100\n",
            "120/120 [==============================] - 0s 99us/step - loss: 0.3448 - acc: 0.9583\n",
            "Epoch 42/100\n",
            "120/120 [==============================] - 0s 92us/step - loss: 0.3371 - acc: 0.9583\n",
            "Epoch 43/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.3295 - acc: 0.9583\n",
            "Epoch 44/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.3219 - acc: 0.9583\n",
            "Epoch 45/100\n",
            "120/120 [==============================] - 0s 91us/step - loss: 0.3150 - acc: 0.9583\n",
            "Epoch 46/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.3083 - acc: 0.9583\n",
            "Epoch 47/100\n",
            "120/120 [==============================] - 0s 76us/step - loss: 0.3012 - acc: 0.9583\n",
            "Epoch 48/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.2952 - acc: 0.9583\n",
            "Epoch 49/100\n",
            "120/120 [==============================] - 0s 66us/step - loss: 0.2887 - acc: 0.9667\n",
            "Epoch 50/100\n",
            "120/120 [==============================] - 0s 73us/step - loss: 0.2834 - acc: 0.9583\n",
            "Epoch 51/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.2764 - acc: 0.9583\n",
            "Epoch 52/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.2710 - acc: 0.9583\n",
            "Epoch 53/100\n",
            "120/120 [==============================] - 0s 89us/step - loss: 0.2656 - acc: 0.9667\n",
            "Epoch 54/100\n",
            "120/120 [==============================] - 0s 88us/step - loss: 0.2589 - acc: 0.9583\n",
            "Epoch 55/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.2531 - acc: 0.9667\n",
            "Epoch 56/100\n",
            "120/120 [==============================] - 0s 74us/step - loss: 0.2484 - acc: 0.9583\n",
            "Epoch 57/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.2435 - acc: 0.9583\n",
            "Epoch 58/100\n",
            "120/120 [==============================] - 0s 109us/step - loss: 0.2395 - acc: 0.9583\n",
            "Epoch 59/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.2329 - acc: 0.9583\n",
            "Epoch 60/100\n",
            "120/120 [==============================] - 0s 90us/step - loss: 0.2281 - acc: 0.9583\n",
            "Epoch 61/100\n",
            "120/120 [==============================] - 0s 113us/step - loss: 0.2240 - acc: 0.9667\n",
            "Epoch 62/100\n",
            "120/120 [==============================] - 0s 100us/step - loss: 0.2204 - acc: 0.9667\n",
            "Epoch 63/100\n",
            "120/120 [==============================] - 0s 94us/step - loss: 0.2149 - acc: 0.9583\n",
            "Epoch 64/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.2112 - acc: 0.9583\n",
            "Epoch 65/100\n",
            "120/120 [==============================] - 0s 80us/step - loss: 0.2080 - acc: 0.9667\n",
            "Epoch 66/100\n",
            "120/120 [==============================] - 0s 71us/step - loss: 0.2030 - acc: 0.9667\n",
            "Epoch 67/100\n",
            "120/120 [==============================] - 0s 88us/step - loss: 0.1999 - acc: 0.9500\n",
            "Epoch 68/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.1957 - acc: 0.9583\n",
            "Epoch 69/100\n",
            "120/120 [==============================] - 0s 88us/step - loss: 0.1912 - acc: 0.9583\n",
            "Epoch 70/100\n",
            "120/120 [==============================] - 0s 112us/step - loss: 0.1884 - acc: 0.9667\n",
            "Epoch 71/100\n",
            "120/120 [==============================] - 0s 77us/step - loss: 0.1856 - acc: 0.9667\n",
            "Epoch 72/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.1816 - acc: 0.9667\n",
            "Epoch 73/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.1796 - acc: 0.9667\n",
            "Epoch 74/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.1774 - acc: 0.9583\n",
            "Epoch 75/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.1729 - acc: 0.9583\n",
            "Epoch 76/100\n",
            "120/120 [==============================] - 0s 70us/step - loss: 0.1699 - acc: 0.9583\n",
            "Epoch 77/100\n",
            "120/120 [==============================] - 0s 69us/step - loss: 0.1667 - acc: 0.9583\n",
            "Epoch 78/100\n",
            "120/120 [==============================] - 0s 70us/step - loss: 0.1653 - acc: 0.9583\n",
            "Epoch 79/100\n",
            "120/120 [==============================] - 0s 82us/step - loss: 0.1626 - acc: 0.9583\n",
            "Epoch 80/100\n",
            "120/120 [==============================] - 0s 159us/step - loss: 0.1612 - acc: 0.9583\n",
            "Epoch 81/100\n",
            "120/120 [==============================] - 0s 130us/step - loss: 0.1590 - acc: 0.9667\n",
            "Epoch 82/100\n",
            "120/120 [==============================] - 0s 116us/step - loss: 0.1548 - acc: 0.9583\n",
            "Epoch 83/100\n",
            "120/120 [==============================] - 0s 110us/step - loss: 0.1525 - acc: 0.9667\n",
            "Epoch 84/100\n",
            "120/120 [==============================] - 0s 98us/step - loss: 0.1508 - acc: 0.9583\n",
            "Epoch 85/100\n",
            "120/120 [==============================] - 0s 96us/step - loss: 0.1490 - acc: 0.9583\n",
            "Epoch 86/100\n",
            "120/120 [==============================] - 0s 104us/step - loss: 0.1473 - acc: 0.9583\n",
            "Epoch 87/100\n",
            "120/120 [==============================] - 0s 97us/step - loss: 0.1443 - acc: 0.9583\n",
            "Epoch 88/100\n",
            "120/120 [==============================] - 0s 78us/step - loss: 0.1431 - acc: 0.9583\n",
            "Epoch 89/100\n",
            "120/120 [==============================] - 0s 83us/step - loss: 0.1406 - acc: 0.9667\n",
            "Epoch 90/100\n",
            "120/120 [==============================] - 0s 88us/step - loss: 0.1390 - acc: 0.9667\n",
            "Epoch 91/100\n",
            "120/120 [==============================] - 0s 79us/step - loss: 0.1368 - acc: 0.9667\n",
            "Epoch 92/100\n",
            "120/120 [==============================] - 0s 73us/step - loss: 0.1362 - acc: 0.9583\n",
            "Epoch 93/100\n",
            "120/120 [==============================] - 0s 84us/step - loss: 0.1346 - acc: 0.9667\n",
            "Epoch 94/100\n",
            "120/120 [==============================] - 0s 87us/step - loss: 0.1342 - acc: 0.9583\n",
            "Epoch 95/100\n",
            "120/120 [==============================] - 0s 93us/step - loss: 0.1313 - acc: 0.9583\n",
            "Epoch 96/100\n",
            "120/120 [==============================] - 0s 92us/step - loss: 0.1303 - acc: 0.9667\n",
            "Epoch 97/100\n",
            "120/120 [==============================] - 0s 102us/step - loss: 0.1284 - acc: 0.9583\n",
            "Epoch 98/100\n",
            "120/120 [==============================] - 0s 79us/step - loss: 0.1272 - acc: 0.9667\n",
            "Epoch 99/100\n",
            "120/120 [==============================] - 0s 118us/step - loss: 0.1251 - acc: 0.9667\n",
            "Epoch 100/100\n",
            "120/120 [==============================] - 0s 108us/step - loss: 0.1237 - acc: 0.9583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7bad268438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et2CA-FbZVX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "31d10f8a-770f-4b28-f71c-1b530f680c6e"
      },
      "source": [
        "y_pred = model1.predict(X_test)\n",
        "\n",
        "y_test_class = np.argmax(y_test,axis=1)\n",
        "y_pred_class = np.argmax(y_pred,axis=1)\n",
        "\n",
        "print(y_test_class)\n",
        "print(y_pred_class)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 1 1 1 1 0 0 1 1 2 1 1 2 0 2 2 1 0 1 1 0 0 1 2 2 1 2 2]\n",
            "[0 0 1 1 1 1 1 0 0 1 1 2 1 1 2 0 2 2 1 0 1 1 0 0 1 2 2 1 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKHE_xc_G4PS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "445b3a0a-ade1-4c88-dcd2-3132f2a13e48"
      },
      "source": [
        "print(model1.get_weights())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[ 0.13856044,  0.3324899 , -0.21610937,  0.04943543,  0.19390005,\n",
            "        -0.06632356,  0.3554743 , -0.12060726, -0.13560684,  0.08320681,\n",
            "        -0.2818772 ,  0.12545374,  0.04158588, -0.27185458,  0.21667124,\n",
            "         0.11321702],\n",
            "       [ 0.33275363, -0.02664353, -0.07806253, -0.35604656,  0.06843536,\n",
            "        -0.26292774, -0.18677421, -0.16126104, -0.00232079,  0.3875494 ,\n",
            "         0.07276726,  0.43924507, -0.20629838,  0.57715994, -0.63067955,\n",
            "        -0.58064085],\n",
            "       [-0.22417207, -0.1754693 ,  0.2680888 , -0.07854957,  0.519101  ,\n",
            "        -0.10506185, -0.48912412,  0.05555359,  0.34044474, -0.40393162,\n",
            "         0.17640814, -0.2604668 ,  0.21597   , -0.24550125,  0.17629659,\n",
            "        -0.50271755],\n",
            "       [-0.5925389 , -0.05064395, -0.34189084, -0.1520825 ,  0.40897936,\n",
            "         0.6024598 ,  0.19742891,  0.65534985,  0.5925363 , -0.04908391,\n",
            "        -0.2175926 , -0.6181363 ,  0.5516789 , -0.00600267,  0.1963614 ,\n",
            "        -0.57839704]], dtype=float32), array([ 0.13742824,  0.04824468,  0.05439523, -0.05703732,  0.06185424,\n",
            "       -0.02547296,  0.11995489, -0.02213161, -0.20914172, -0.11135859,\n",
            "        0.10340369,  0.04052618, -0.3119501 ,  0.07609019, -0.14216228,\n",
            "       -0.0842482 ], dtype=float32), array([[-3.17166485e-02,  1.21337272e-01,  4.17395741e-01,\n",
            "         4.88441527e-01, -2.61819839e-01, -8.23463649e-02,\n",
            "        -5.25006711e-01,  1.27660558e-01, -8.06880146e-02,\n",
            "        -4.25691754e-01,  1.20967522e-01, -5.40054142e-01,\n",
            "         5.12546659e-01,  4.43466008e-02, -8.16801265e-02,\n",
            "         6.79953992e-02, -2.56511986e-01, -1.17837831e-01,\n",
            "         2.41292372e-01, -3.96328866e-01,  2.78639287e-01,\n",
            "         4.02984232e-01,  8.33095685e-02,  3.10444474e-01,\n",
            "         2.42591202e-01,  1.37750596e-01,  1.56552531e-02,\n",
            "        -1.11887902e-01, -2.29470044e-01,  5.86789072e-01,\n",
            "         3.94582421e-01,  3.13107073e-01],\n",
            "       [ 2.89024532e-01, -1.76343083e-01,  3.93247902e-01,\n",
            "         1.19458273e-01, -2.68757343e-02,  1.02786561e-02,\n",
            "         4.73655984e-02, -1.23280525e-01, -2.84228295e-01,\n",
            "        -1.66618690e-01, -1.16316251e-01, -3.05591881e-01,\n",
            "         2.94069201e-01, -1.14518985e-01, -2.61401087e-01,\n",
            "        -1.48862645e-01,  1.80032372e-01,  9.69727188e-02,\n",
            "        -8.08520592e-04, -1.25874147e-01,  2.07861140e-01,\n",
            "        -2.53688127e-01,  1.05998203e-01,  3.43179166e-01,\n",
            "        -1.00558065e-01, -1.58939198e-01, -3.22088897e-01,\n",
            "        -2.02019163e-03,  9.72149298e-02,  1.54876247e-01,\n",
            "        -2.43117958e-01,  1.51876882e-01],\n",
            "       [ 3.34972106e-02, -9.82557237e-02,  2.11825818e-01,\n",
            "        -2.67486036e-01, -6.33178875e-02, -3.66664559e-01,\n",
            "        -3.37591916e-01, -1.46252543e-01, -1.35973155e-01,\n",
            "        -3.98255527e-01, -9.85661596e-02,  1.68132976e-01,\n",
            "         3.35863024e-01,  2.28530511e-01, -3.68687570e-01,\n",
            "        -2.48905599e-01, -7.81573728e-02, -1.96105078e-01,\n",
            "        -2.34142449e-02, -1.93055794e-01, -7.87841454e-02,\n",
            "         3.15810814e-02, -1.67185217e-01,  1.25216827e-01,\n",
            "         2.90375739e-01, -1.70601815e-01, -3.10242772e-01,\n",
            "         2.78359711e-01, -7.69737586e-02,  2.08518744e-01,\n",
            "        -9.00378078e-02,  3.37477207e-01],\n",
            "       [-2.74741650e-01,  2.44022056e-01, -3.15405041e-01,\n",
            "        -5.95168099e-02,  2.44330261e-02,  2.93802828e-01,\n",
            "        -3.31294946e-02, -2.58850396e-01,  8.56655650e-03,\n",
            "         1.79563150e-01,  2.92358726e-01, -5.35169281e-02,\n",
            "        -9.49636176e-02, -2.36486211e-01,  1.45658880e-01,\n",
            "         2.20382493e-02, -5.20304181e-02, -2.12079689e-01,\n",
            "        -6.90974742e-02, -3.55497710e-02, -2.24448875e-01,\n",
            "        -2.38955051e-01,  1.01570636e-01, -2.66230609e-02,\n",
            "        -1.65715486e-01,  3.23074614e-03, -2.31752366e-01,\n",
            "        -2.66903520e-01,  8.46000314e-02, -1.76214688e-02,\n",
            "        -1.36518791e-01, -1.25292152e-01],\n",
            "       [ 1.00761995e-01,  1.52276367e-01,  1.88097849e-01,\n",
            "        -1.90467060e-01, -2.22020283e-01,  2.14282930e-01,\n",
            "        -1.46799400e-01, -5.20596690e-02, -1.68020591e-01,\n",
            "         7.05390573e-02, -1.36781499e-01, -1.87570110e-01,\n",
            "        -1.83013961e-01, -4.52085286e-02, -1.48616120e-01,\n",
            "         3.41941565e-01, -2.03137144e-01, -1.30728409e-01,\n",
            "        -3.69669050e-01,  8.97882599e-03,  2.39609256e-01,\n",
            "         2.84339160e-01, -3.00348103e-01,  9.75828245e-02,\n",
            "        -1.16378710e-01,  7.02110380e-02,  2.15782598e-01,\n",
            "         8.15717652e-02,  1.74283013e-01,  6.70210570e-02,\n",
            "         3.09327751e-01, -7.49401003e-02],\n",
            "       [ 2.83090115e-01,  3.19511513e-04, -1.54980093e-01,\n",
            "         4.93559651e-02,  2.17927396e-02,  1.84396178e-01,\n",
            "         2.32408598e-01,  3.48379225e-01, -2.68620431e-01,\n",
            "         1.79194406e-01, -2.98055589e-01,  2.30795681e-01,\n",
            "        -2.00959563e-01,  3.33269536e-02,  6.20430484e-02,\n",
            "         2.26678342e-01,  1.12405308e-01,  1.65506035e-01,\n",
            "         2.18593478e-01,  2.26794541e-01,  2.65142441e-01,\n",
            "         1.92805603e-01,  3.24733913e-01,  6.31371886e-02,\n",
            "        -6.18186444e-02,  2.08344385e-01,  3.24969500e-01,\n",
            "        -2.73274686e-02, -6.75161928e-02,  8.59235600e-02,\n",
            "        -1.71357006e-01,  1.86078041e-03],\n",
            "       [ 1.90200180e-01,  6.91504851e-02, -1.41866682e-02,\n",
            "         3.56621385e-01, -1.96735755e-01, -5.24331704e-02,\n",
            "         2.39647359e-01, -2.11595669e-01, -7.20107630e-02,\n",
            "        -2.63712883e-01,  4.73194152e-01, -3.13285381e-01,\n",
            "         1.37158960e-01,  9.22408327e-02,  4.00407732e-01,\n",
            "        -4.20473397e-01, -4.67569858e-01,  2.21612826e-01,\n",
            "        -1.51476085e-01, -6.60286695e-02, -1.01175644e-01,\n",
            "         5.74402273e-01,  2.05648884e-01, -1.53887659e-01,\n",
            "        -6.29726797e-02,  4.03140187e-01, -1.40661627e-01,\n",
            "        -1.29072845e-01, -3.69957238e-01,  4.43236768e-01,\n",
            "         5.63410461e-01, -9.99287069e-02],\n",
            "       [ 1.53322443e-01, -2.29301870e-01, -4.20310438e-01,\n",
            "        -3.36230218e-01,  8.80802423e-02,  4.48838025e-02,\n",
            "         3.91125739e-01,  1.84008330e-01,  5.29429317e-01,\n",
            "         2.01744735e-01, -4.22165841e-01, -1.44298607e-02,\n",
            "        -2.02892512e-01, -1.98458016e-01,  2.72620887e-01,\n",
            "        -1.82882577e-01,  4.16861661e-03,  2.22964108e-01,\n",
            "        -9.50212255e-02, -8.72936158e-04,  9.52666402e-02,\n",
            "         1.75650358e-01, -3.53169441e-01,  1.50114506e-01,\n",
            "        -1.54748201e-01, -2.91789383e-01, -2.13082001e-01,\n",
            "         2.93844670e-01, -8.92531052e-02, -1.13024957e-01,\n",
            "        -6.11419901e-02, -9.07491520e-02],\n",
            "       [-2.78474748e-01, -2.06778616e-01, -1.05717011e-01,\n",
            "         2.37196907e-02,  3.34924906e-01,  6.05519861e-02,\n",
            "        -1.84222072e-01,  2.07527235e-01, -2.30189845e-01,\n",
            "         1.82536110e-01,  2.96703756e-01,  1.38088420e-01,\n",
            "         1.13879390e-01, -4.07380342e-01,  1.80823028e-01,\n",
            "         2.53135860e-01, -1.18950300e-01, -1.26451850e-01,\n",
            "        -4.27354634e-01, -3.10803466e-02,  2.05779016e-01,\n",
            "        -1.76422238e-01,  1.16305128e-01, -1.50176048e-01,\n",
            "         3.48247021e-01, -2.41164714e-01, -1.54525191e-01,\n",
            "         2.57510722e-01,  3.65813822e-01,  2.04336986e-01,\n",
            "         2.22107083e-01, -4.66634631e-02],\n",
            "       [-2.19132468e-01,  1.67005323e-02, -9.96935368e-02,\n",
            "         5.61188199e-02,  2.40527749e-01,  4.01874259e-02,\n",
            "        -1.11319542e-01, -1.88093990e-01, -2.60471046e-01,\n",
            "        -3.13307822e-01, -5.07300645e-02, -3.58554661e-01,\n",
            "        -2.80578192e-02,  3.74079078e-01,  1.85103089e-01,\n",
            "        -2.12334648e-01, -1.49391443e-01, -4.51873839e-02,\n",
            "         2.73805022e-01, -2.15996206e-01, -1.22730181e-01,\n",
            "        -5.58097623e-02, -1.82492986e-01, -3.80750328e-01,\n",
            "        -1.93337008e-01,  5.18594906e-02,  2.61448771e-01,\n",
            "        -1.95433095e-01, -3.92597029e-03,  9.72360745e-02,\n",
            "        -2.26387322e-01,  3.17504227e-01],\n",
            "       [-2.86548764e-01,  2.43737221e-01,  3.44204195e-02,\n",
            "        -3.33438963e-01, -1.92902312e-01, -1.39405206e-01,\n",
            "         2.83734947e-01,  2.07206383e-01,  1.02699287e-01,\n",
            "         1.71767890e-01,  8.48444477e-02, -1.20403588e-01,\n",
            "         2.92259157e-01,  8.50002244e-02, -2.06329212e-01,\n",
            "        -2.96006739e-01,  2.32755095e-02,  1.45658135e-01,\n",
            "        -1.92589998e-01, -1.98563695e-01,  2.89646477e-01,\n",
            "         2.56199092e-01,  3.33265513e-01,  4.13903559e-04,\n",
            "         1.70081437e-01, -1.57411873e-01, -2.31885299e-01,\n",
            "        -2.39644244e-01,  1.32863879e-01, -2.39914060e-02,\n",
            "         2.33201787e-01, -2.17421532e-01],\n",
            "       [ 4.18281823e-01,  5.59529150e-03,  3.87349367e-01,\n",
            "         5.36095798e-01, -4.42865402e-01, -2.66251061e-02,\n",
            "        -5.47769845e-01, -4.57277596e-01, -2.55117118e-01,\n",
            "        -4.98236381e-02,  1.88368917e-01, -5.50407469e-01,\n",
            "        -9.76600498e-02,  3.69284660e-01, -2.01345623e-01,\n",
            "        -1.57170847e-01, -3.49431813e-01,  2.10034400e-01,\n",
            "         3.48408133e-01, -6.33693039e-02,  1.38427243e-01,\n",
            "         4.39914137e-01,  9.67458412e-02, -7.52727687e-02,\n",
            "        -1.47131741e-01,  5.65442204e-01,  2.24113196e-01,\n",
            "        -3.57985616e-01, -1.54818043e-01,  4.58953887e-01,\n",
            "         3.94849598e-01, -2.27873735e-02],\n",
            "       [-1.50103569e-02,  3.47064495e-01, -1.37571111e-01,\n",
            "        -2.84139872e-01,  2.41774395e-01, -3.33663881e-01,\n",
            "         1.88794971e-01,  1.56735275e-02,  3.37603450e-01,\n",
            "         3.12891066e-01, -1.76038980e-01,  2.34551996e-01,\n",
            "         5.90010211e-02,  5.50285503e-02,  1.90017283e-01,\n",
            "        -1.74250349e-01,  1.53781906e-01, -1.08089447e-01,\n",
            "         1.27892837e-01, -2.32437491e-01,  7.82834888e-02,\n",
            "        -6.01593219e-02,  3.83173168e-01,  2.97989428e-01,\n",
            "         2.79109657e-01,  1.97214693e-01, -5.87167963e-02,\n",
            "         3.53639901e-01,  1.57422408e-01, -1.04901612e-01,\n",
            "         6.31198585e-02,  4.10829782e-02],\n",
            "       [ 8.49283561e-02,  8.76617357e-02,  8.66921842e-02,\n",
            "         1.19237989e-01, -5.08829415e-01, -3.97691280e-02,\n",
            "         2.47560993e-01, -9.70197916e-02,  1.43443614e-01,\n",
            "         1.06752247e-01,  1.32923082e-01,  8.36386606e-02,\n",
            "        -1.13126777e-01,  9.97499451e-02,  1.10568970e-01,\n",
            "        -1.15394041e-01, -1.54965907e-01,  3.37689042e-01,\n",
            "        -2.32065357e-02, -8.02977532e-02,  4.53890115e-02,\n",
            "         5.71622014e-01, -2.87438273e-01, -4.32113856e-01,\n",
            "        -4.21242803e-01, -7.68126920e-03, -9.36193764e-02,\n",
            "        -3.11766326e-01, -5.29299259e-01, -8.80079791e-02,\n",
            "         3.31215113e-01, -1.53416544e-01],\n",
            "       [-2.95751095e-02, -4.70301092e-01, -3.80790681e-01,\n",
            "         1.89666465e-01,  8.54040608e-02,  1.51626498e-01,\n",
            "         2.31745571e-01,  1.21939331e-01,  3.19223404e-01,\n",
            "         4.04480875e-01, -9.49555561e-02, -3.13615739e-01,\n",
            "        -4.33232099e-01, -2.08877213e-02, -3.72535497e-01,\n",
            "         7.28077143e-02,  4.59161311e-01, -4.73300606e-01,\n",
            "        -4.83953595e-01,  1.11775845e-01, -1.93984464e-01,\n",
            "         5.06090112e-02, -4.33486477e-02,  4.61432159e-01,\n",
            "        -1.45305157e-01,  5.25940359e-02,  1.89438798e-02,\n",
            "         3.25492769e-01,  4.15938467e-01, -1.77011147e-01,\n",
            "        -4.73088712e-01, -1.29768148e-01],\n",
            "       [-2.84297407e-01,  1.17232464e-01,  1.98249474e-01,\n",
            "         4.28997204e-02,  9.08302069e-02,  1.95192739e-01,\n",
            "        -1.50266951e-02,  5.82010075e-02,  2.45693535e-01,\n",
            "         2.10542127e-01, -2.39209205e-01, -2.02181578e-01,\n",
            "        -1.50408551e-01,  1.84681848e-01, -7.61940777e-02,\n",
            "         3.50632697e-01, -4.97163460e-02, -1.58631265e-01,\n",
            "         2.55820811e-01,  7.50894845e-02, -8.72084498e-02,\n",
            "         2.91732520e-01,  1.71862110e-01,  2.11971343e-01,\n",
            "         1.53525710e-01, -2.24122763e-01,  3.01168948e-01,\n",
            "         2.18341276e-01,  2.12968677e-01, -9.47155803e-02,\n",
            "        -1.89759657e-01,  2.94222891e-01]], dtype=float32), array([-0.01511411,  0.08324499,  0.05204548,  0.10614762,  0.02894598,\n",
            "       -0.01111527, -0.00395838, -0.00048392,  0.03179271,  0.07282607,\n",
            "        0.0503342 , -0.11895213, -0.00160426, -0.00434447, -0.01747549,\n",
            "       -0.00515572,  0.02700841,  0.0125754 , -0.04364839,  0.00614906,\n",
            "       -0.01002025, -0.00557723,  0.01844248,  0.0133923 ,  0.01846245,\n",
            "        0.00477437,  0.0026343 ,  0.07240248,  0.02541468,  0.10210163,\n",
            "       -0.03365758,  0.00745536], dtype=float32), array([[ 0.04319751,  0.07363351, -0.09524048],\n",
            "       [ 0.25042552, -0.06959346, -0.3376938 ],\n",
            "       [ 0.15387367,  0.45493728, -0.56106347],\n",
            "       [-0.06668454, -0.11187852, -0.602664  ],\n",
            "       [-0.33767608,  0.37911457,  0.374731  ],\n",
            "       [ 0.20195755, -0.2540596 ,  0.16587849],\n",
            "       [ 0.19285098, -0.42213574,  0.29850823],\n",
            "       [-0.41267055, -0.05814252,  0.37985304],\n",
            "       [-0.1566011 , -0.07289796,  0.4235622 ],\n",
            "       [-0.33799368,  0.21487963,  0.46872503],\n",
            "       [ 0.63282937,  0.41441032, -0.3203656 ],\n",
            "       [ 0.19701846,  0.06548486,  0.44021463],\n",
            "       [ 0.5071997 ,  0.29498792, -0.37909806],\n",
            "       [ 0.3080363 , -0.36647865, -0.19954905],\n",
            "       [ 0.17947526, -0.57908773, -0.10787699],\n",
            "       [-0.38899526, -0.2781311 ,  0.2509225 ],\n",
            "       [ 0.05632134,  0.4186254 ,  0.5421804 ],\n",
            "       [ 0.43559015,  0.01128229,  0.11107905],\n",
            "       [ 0.2869729 , -0.5299361 ,  0.0576371 ],\n",
            "       [-0.15671128, -0.27653942,  0.2835073 ],\n",
            "       [ 0.31709635,  0.09083898,  0.1127711 ],\n",
            "       [ 0.13304825, -0.29743177, -0.12998994],\n",
            "       [-0.13878495,  0.32015854,  0.02765482],\n",
            "       [-0.19419104,  0.24657503,  0.02246572],\n",
            "       [-0.4370423 ,  0.5323348 ,  0.31042883],\n",
            "       [ 0.2859139 ,  0.4525087 , -0.37837076],\n",
            "       [ 0.21668199,  0.16700573, -0.17793675],\n",
            "       [-0.5833681 ,  0.2414152 ,  0.0764682 ],\n",
            "       [-0.0530798 ,  0.17704594,  0.13390303],\n",
            "       [ 0.3583561 ,  0.49525827, -0.13539478],\n",
            "       [ 0.51201147,  0.39099187, -0.25275335],\n",
            "       [ 0.28633747,  0.5164478 , -0.3088804 ]], dtype=float32), array([-0.00889346, -0.00374222,  0.0087546 ], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_S_N6tpaAnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_NN(model): # add random vector to weights for mutation\n",
        "  weights = model.get_weights()\n",
        "  for i in range(len(weights)):\n",
        "    weights[i] = np.random.uniform(-1,1,weights[i].shape)\n",
        "  new_model = build_model()\n",
        "  new_model.set_weights(weights)\n",
        "  return new_model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pTfd5_rbPz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "57071436-ed57-453f-e5e1-7849c75d7337"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "new_model = update_NN(model1)\n",
        "y_pred = new_model.predict(X_test)\n",
        "\n",
        "y_test_class = np.argmax(y_test,axis=1)\n",
        "y_pred_class = np.argmax(y_pred,axis=1)\n",
        "\n",
        "print(y_test_class)\n",
        "print(y_pred_class)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 1 1 1 1 0 0 1 1 2 1 1 2 0 2 2 1 0 1 1 0 0 1 2 2 1 2 2]\n",
            "[0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBHacs8Pc8M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def energy_fun(model):  # cost function using MSE\n",
        "  y_pred = model.predict(X_train)\n",
        "  mse = (np.square(y_pred - y_train)).mean(axis=None)\n",
        "  return mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68bCY7hudaQ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3559a87e-a938-461b-8f3f-92e5cd497801"
      },
      "source": [
        "energy_fun(new_model) # result of mutated model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22212753"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64HmXAgEdc5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49d6cd60-8126-42d7-9db3-2f7654091e19"
      },
      "source": [
        "energy_fun(model1) # result of backprop"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.019370927"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxA5rgm3RYyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simulated_annealing(initial, generate_f, energy_f, T = 2 , alpha = 0.1, iters = 10, min_T = 0.05):\n",
        "  best = initial\n",
        "  e_best = energy_f(initial)\n",
        "  x0 = initial\n",
        "  while T > min_T:\n",
        "    for i in range(iters):\n",
        "      x = generate_f(x0)\n",
        "      e_x = energy_f(x)\n",
        "      e_x0 = energy_f(x0)\n",
        "      if(e_x < e_x0):\n",
        "        x0 = x\n",
        "        if e_x < e_best:\n",
        "          e_best = e_x\n",
        "          best = x\n",
        "      else:\n",
        "        accept_ratio = np.exp(-e_x/T)/ np.exp(-e_x0/T)\n",
        "        if accept_ratio < np.random.uniform(0.0,1.0):\n",
        "          x0 = x\n",
        "      print(T, i, e_best, e_x, e_x0)\n",
        "    T = T * alpha\n",
        "  return best,e_best\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozYKNL6GdvU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model):\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  y_test_class = np.argmax(y_test,axis=1)\n",
        "  y_pred_class = np.argmax(y_pred,axis=1)\n",
        "  return accuracy_score(y_test_class, y_pred_class)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTGmaMbJdj1y",
        "colab_type": "code",
        "outputId": "3b75a108-761a-4dce-820f-f77fd7279648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# run 1\n",
        "initial = build_model()\n",
        "T = 10.0\n",
        "alpha = 0.80\n",
        "T_min = 1\n",
        "iters = 5\n",
        "\n",
        "model,cost  = simulated_annealing(initial=initial,generate_f=update_NN, energy_f=energy_fun,T = 5)\n",
        "\n",
        "cost"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 0 0.2764798 0.3273315 0.2764798\n",
            "5 1 0.2764798 0.37411362 0.2764798\n",
            "5 2 0.2764798 0.38740367 0.2764798\n",
            "5 3 0.2764798 0.42791247 0.2764798\n",
            "5 4 0.2764798 0.36527464 0.2764798\n",
            "5 5 0.2764798 0.3648541 0.2764798\n",
            "5 6 0.23668188 0.23668188 0.2764798\n",
            "5 7 0.2239177 0.2239177 0.23668188\n",
            "5 8 0.2239177 0.44255167 0.2239177\n",
            "5 9 0.2239177 0.4136252 0.2239177\n",
            "0.5 0 0.2239177 0.38258404 0.2239177\n",
            "0.5 1 0.2239177 0.34018785 0.2239177\n",
            "0.5 2 0.2239177 0.30889025 0.2239177\n",
            "0.5 3 0.2239177 0.2635761 0.2239177\n",
            "0.5 4 0.21505643 0.21505643 0.2239177\n",
            "0.5 5 0.19535199 0.19535199 0.21505643\n",
            "0.5 6 0.19535199 0.23984618 0.19535199\n",
            "0.5 7 0.19535199 0.31433225 0.19535199\n",
            "0.5 8 0.19535199 0.42101982 0.31433225\n",
            "0.5 9 0.19535199 0.46287563 0.42101982\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19535199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD_WcgwTeBCX",
        "colab_type": "code",
        "outputId": "23ab146a-1579-46ad-bbf2-55267d347cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(model)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD1p35LxKoOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "9a7b696f-742a-459c-f41f-67374fab875e"
      },
      "source": [
        "# run 2\n",
        "initial = build_model()\n",
        "T = 1000.0\n",
        "alpha = 0.5\n",
        "T_min = 10\n",
        "iters = 5\n",
        "\n",
        "model,cost  = simulated_annealing(initial=initial,generate_f=update_NN, energy_f=energy_fun,T = T, min_T = T_min, iters = iters, alpha = alpha)\n",
        "\n",
        "cost"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000.0 0 0.26613054 0.4388989 0.26613054\n",
            "1000.0 1 0.26613054 0.28311107 0.26613054\n",
            "1000.0 2 0.26613054 0.44648573 0.26613054\n",
            "1000.0 3 0.26613054 0.4550956 0.26613054\n",
            "1000.0 4 0.26613054 0.43208963 0.26613054\n",
            "500.0 0 0.26613054 0.41384202 0.26613054\n",
            "500.0 1 0.15144482 0.15144482 0.26613054\n",
            "500.0 2 0.15144482 0.40087196 0.15144482\n",
            "500.0 3 0.15144482 0.34181944 0.15144482\n",
            "500.0 4 0.15144482 0.41541016 0.15144482\n",
            "250.0 0 0.15144482 0.34895557 0.15144482\n",
            "250.0 1 0.15144482 0.36676848 0.15144482\n",
            "250.0 2 0.15144482 0.3661289 0.15144482\n",
            "250.0 3 0.15144482 0.31883895 0.15144482\n",
            "250.0 4 0.15144482 0.41973814 0.15144482\n",
            "125.0 0 0.15144482 0.45269626 0.15144482\n",
            "125.0 1 0.15144482 0.39894623 0.15144482\n",
            "125.0 2 0.15144482 0.36905858 0.15144482\n",
            "125.0 3 0.15144482 0.330976 0.15144482\n",
            "125.0 4 0.15144482 0.43948132 0.15144482\n",
            "62.5 0 0.15144482 0.3747885 0.15144482\n",
            "62.5 1 0.15144482 0.41568568 0.15144482\n",
            "62.5 2 0.15144482 0.335385 0.15144482\n",
            "62.5 3 0.15144482 0.24236794 0.15144482\n",
            "62.5 4 0.15144482 0.30799687 0.15144482\n",
            "31.25 0 0.15144482 0.29337397 0.15144482\n",
            "31.25 1 0.15144482 0.42155364 0.15144482\n",
            "31.25 2 0.15144482 0.4661466 0.15144482\n",
            "31.25 3 0.15144482 0.37089512 0.15144482\n",
            "31.25 4 0.15144482 0.22836149 0.15144482\n",
            "15.625 0 0.15144482 0.31581688 0.15144482\n",
            "15.625 1 0.15144482 0.23278864 0.15144482\n",
            "15.625 2 0.15144482 0.42158848 0.15144482\n",
            "15.625 3 0.15144482 0.31503022 0.15144482\n",
            "15.625 4 0.15144482 0.26630834 0.15144482\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15144482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7S1tnFZeR02",
        "colab_type": "code",
        "outputId": "c0f815b9-8372-4424-96f7-d0337f86f0d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhJDuCnSeVRi",
        "colab_type": "code",
        "outputId": "265050eb-495d-4f9f-ec77-43089780647f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# run 3\n",
        "initial = build_model()\n",
        "T = 5.0\n",
        "alpha = 0.8\n",
        "T_min = 1\n",
        "iters = 10\n",
        "\n",
        "model,cost  = simulated_annealing(initial=initial,generate_f=update_NN, energy_f=energy_fun,T = T, min_T = T_min, iters = iters, alpha = alpha)\n",
        "\n",
        "cost"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0 0 0.22501716 0.42033005 0.22501716\n",
            "5.0 1 0.22501716 0.36543468 0.22501716\n",
            "5.0 2 0.21971859 0.21971859 0.22501716\n",
            "5.0 3 0.21971859 0.29321182 0.21971859\n",
            "5.0 4 0.21971859 0.3507562 0.21971859\n",
            "5.0 5 0.08423306 0.08423306 0.21971859\n",
            "5.0 6 0.08423306 0.42129165 0.08423306\n",
            "5.0 7 0.08423306 0.42522514 0.08423306\n",
            "5.0 8 0.08423306 0.57941854 0.08423306\n",
            "5.0 9 0.08423306 0.37934792 0.08423306\n",
            "4.0 0 0.08423306 0.45700046 0.37934792\n",
            "4.0 1 0.08423306 0.41588816 0.37934792\n",
            "4.0 2 0.08423306 0.34075528 0.37934792\n",
            "4.0 3 0.08423306 0.40152472 0.34075528\n",
            "4.0 4 0.08423306 0.33312684 0.34075528\n",
            "4.0 5 0.08423306 0.39693433 0.33312684\n",
            "4.0 6 0.08423306 0.27930564 0.33312684\n",
            "4.0 7 0.08423306 0.45494097 0.27930564\n",
            "4.0 8 0.08423306 0.3265855 0.27930564\n",
            "4.0 9 0.08423306 0.34509256 0.27930564\n",
            "3.2 0 0.08423306 0.5665856 0.27930564\n",
            "3.2 1 0.08423306 0.41481322 0.27930564\n",
            "3.2 2 0.08423306 0.2960132 0.27930564\n",
            "3.2 3 0.08423306 0.22399732 0.27930564\n",
            "3.2 4 0.08423306 0.32493538 0.22399732\n",
            "3.2 5 0.08423306 0.3276114 0.22399732\n",
            "3.2 6 0.08423306 0.46636048 0.22399732\n",
            "3.2 7 0.08423306 0.3159079 0.22399732\n",
            "3.2 8 0.08423306 0.2292361 0.22399732\n",
            "3.2 9 0.08423306 0.3541104 0.22399732\n",
            "2.5600000000000005 0 0.08423306 0.33987966 0.22399732\n",
            "2.5600000000000005 1 0.08423306 0.36382753 0.22399732\n",
            "2.5600000000000005 2 0.08423306 0.40917966 0.22399732\n",
            "2.5600000000000005 3 0.08423306 0.18967143 0.22399732\n",
            "2.5600000000000005 4 0.08423306 0.40010694 0.18967143\n",
            "2.5600000000000005 5 0.08423306 0.43336952 0.18967143\n",
            "2.5600000000000005 6 0.08423306 0.2600892 0.18967143\n",
            "2.5600000000000005 7 0.08423306 0.39844593 0.18967143\n",
            "2.5600000000000005 8 0.08423306 0.31558356 0.18967143\n",
            "2.5600000000000005 9 0.08423306 0.37289378 0.18967143\n",
            "2.0480000000000005 0 0.08423306 0.26060084 0.18967143\n",
            "2.0480000000000005 1 0.08423306 0.31999537 0.18967143\n",
            "2.0480000000000005 2 0.08423306 0.46634284 0.18967143\n",
            "2.0480000000000005 3 0.08423306 0.3580734 0.18967143\n",
            "2.0480000000000005 4 0.08423306 0.5127997 0.18967143\n",
            "2.0480000000000005 5 0.08423306 0.18444778 0.5127997\n",
            "2.0480000000000005 6 0.08423306 0.38250405 0.18444778\n",
            "2.0480000000000005 7 0.08423306 0.3127087 0.18444778\n",
            "2.0480000000000005 8 0.08423306 0.3847072 0.18444778\n",
            "2.0480000000000005 9 0.08423306 0.389603 0.18444778\n",
            "1.6384000000000005 0 0.08423306 0.36094767 0.18444778\n",
            "1.6384000000000005 1 0.08423306 0.45158988 0.18444778\n",
            "1.6384000000000005 2 0.08423306 0.42735088 0.45158988\n",
            "1.6384000000000005 3 0.08423306 0.34485015 0.42735088\n",
            "1.6384000000000005 4 0.08423306 0.37542585 0.34485015\n",
            "1.6384000000000005 5 0.08423306 0.5908621 0.34485015\n",
            "1.6384000000000005 6 0.08423306 0.29381165 0.34485015\n",
            "1.6384000000000005 7 0.08423306 0.4161989 0.29381165\n",
            "1.6384000000000005 8 0.08423306 0.31114164 0.29381165\n",
            "1.6384000000000005 9 0.08423306 0.20105459 0.29381165\n",
            "1.3107200000000006 0 0.08423306 0.3176735 0.20105459\n",
            "1.3107200000000006 1 0.08423306 0.33749732 0.3176735\n",
            "1.3107200000000006 2 0.08423306 0.33155498 0.3176735\n",
            "1.3107200000000006 3 0.08423306 0.38588825 0.3176735\n",
            "1.3107200000000006 4 0.08423306 0.4542138 0.3176735\n",
            "1.3107200000000006 5 0.08423306 0.37550184 0.4542138\n",
            "1.3107200000000006 6 0.08423306 0.37682995 0.37550184\n",
            "1.3107200000000006 7 0.08423306 0.39961442 0.37550184\n",
            "1.3107200000000006 8 0.08423306 0.24287239 0.37550184\n",
            "1.3107200000000006 9 0.08423306 0.39735335 0.24287239\n",
            "1.0485760000000004 0 0.08423306 0.38136944 0.24287239\n",
            "1.0485760000000004 1 0.08423306 0.41460347 0.38136944\n",
            "1.0485760000000004 2 0.08423306 0.34795925 0.38136944\n",
            "1.0485760000000004 3 0.08423306 0.43131545 0.34795925\n",
            "1.0485760000000004 4 0.08423306 0.32798436 0.34795925\n",
            "1.0485760000000004 5 0.08423306 0.36895564 0.32798436\n",
            "1.0485760000000004 6 0.08423306 0.39295992 0.32798436\n",
            "1.0485760000000004 7 0.08423306 0.42907315 0.32798436\n",
            "1.0485760000000004 8 0.08423306 0.27215713 0.32798436\n",
            "1.0485760000000004 9 0.08423306 0.29011485 0.27215713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08423306"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Dt1j8fGeq-w",
        "colab_type": "code",
        "outputId": "b01f56cb-2b03-4029-fdd1-907d200d8206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRULgAQkELCx",
        "colab_type": "text"
      },
      "source": [
        "#REPORT\n",
        "\n",
        "##Result of backpropagation\n",
        "\n",
        "ACC = 100 %\n",
        "Time - couple of seconds.\n",
        "\n",
        "\n",
        "\n",
        "## Result of \"SA\".\n",
        "\n",
        "### run 1 :\n",
        "\n",
        "T = 5\n",
        "\n",
        "T_min = 0.1\n",
        "\n",
        "alpha = 0.15\n",
        "\n",
        "cost = 0.19\n",
        "\n",
        "acc = 50%\n",
        "\n",
        "Time - couple of minutes\n",
        "\n",
        "### run 2 :\n",
        "T = 1000.0\n",
        "\n",
        "alpha = 0.5\n",
        "\n",
        "T_min = 10\n",
        "\n",
        "iters = 5\n",
        "\n",
        "acc = 53%\n",
        "\n",
        "Time - couple of minutes\n",
        "\n",
        "### run 3 :\n",
        "T = 5\n",
        "\n",
        "alpha = 0.8\n",
        "\n",
        "T_min = 1\n",
        "\n",
        "iters = 10\n",
        "\n",
        "acc = 93%\n",
        "\n",
        "Time - couple of minutes\n",
        "\n",
        "\n",
        "## Result\n",
        "It is easy can be seen, that backpropagation is more suitable for the task of learning NN. However, we can see that SA also could be use and show pretty good result. However takes much more time and very unstable. SA is also more powerful tool and can solve many different tasks of optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIG8cJw5Llli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}